---
title: "Homework 6"
author: "Ayeshra Acharya"
date: "11/24/2020"
output: github_document
---
```{r}
library(tidyverse)
library(p8105.datasets)
library(modelr)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Problem 1 

```{r, message = FALSE}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)
```


Start with one city. 
```{r}
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")
glm(resolution ~ victim_age + victim_race + victim_sex, 
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
``` 
Want to do same process for every city in dataset. How? 
First, nest dataset and then map over that fitting the regression model and then map across regression model. 
We want city estimates of OR and estimates comparing age, race and sex. 

```{r}
models_results_df = 
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI")) 
```
Make a plot of these OR comparing male homicide victims to female homicide victims. 
```{r}
models_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
## Problem 2 
### Load and clean data
```{r, message = FALSE}
birth_df = 
  read.csv("data/birthweight.csv") %>% 
  mutate(
    babysex = as.factor(babysex), 
    mrace = as.factor(mrace),
    malform = as.factor(malform),
    frace = as.factor(frace))
```

### Check for missing values
```{r}
count_na =
birth_df %>%
  map_df(~sum(is.na(.)))
```

There are no NAs in `birth_df` 

I will be using a backward stepwise regression approach, which will begin with a full model and gradually eliminate variables. 

```{r, message = FALSE}
# Full model 
full_model = lm(bwt ~ ., data = birth_df)
step(full_model, direction = "backward")
```

The smallest AIC is 48705.38, which we will call model 1. 
```{r}
model_1 = lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = birth_df)

model_1 %>% 
  broom::tidy() %>% 
  select(-std.error, -statistic) %>% 
  knitr::kable()
```

### Residuals vs predicted 

```{r}
birth_df %>% 
  add_residuals(model_1) %>% 
  add_predictions(model_1) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = 0.5)
```
Based on the plot above, we can see that the points are all nearby each other, scattered around a residual value of 0, with some possible outliers. 

### Comparing to other models 
Now, I will create two models, one which is the main effects and the other with a 3-way interaction. 

```{r}
main_mod = lm(bwt ~ blength + gaweeks, data = birth_df)
interaction_mod = lm(bwt ~ bhead * blength * babysex, data = birth_df)
```

### Cross validation of models 

```{r}
cross_df = 
  crossv_mc(birth_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
cross_df = 
  cross_df %>% 
  mutate(
    model_1 = map(.x = train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
    main_mod = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    interaction_mod = map(.x = train, ~lm(bwt ~ bhead * blength * babysex, data = .x))
  ) %>% 
  mutate(
    rmse_model_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_main_mod = map2_dbl(main_mod, test, ~rmse(model = .x, data = .y)),
    rmse_interaction_mod = map2_dbl(interaction_mod, test, ~rmse(model = .x, data = .y))
    )
```

### Now, I will compare root mean square errors between the models. 

```{r}
cross_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin(aes(fill = model)) 
```
The violin plot comparing the 3 models shows us that model_1 has the lowest RMSE and thus it is the best model. This model also had the lowest AIC, as indicated above. 

## Problem 3 
### Read in data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

### Using 5000 Bootstraps to produce estimates of r̂2 and log(β̂ 0∗β1)

```{r}
bootstrap_results = 
  weather_df %>% 
  modelr::bootstrap(n = 5000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~ lm(tmax ~ tmin, data = .x)),
    r_squared = map(models, broom::glance),
    results = map(models, broom::tidy)
  ) %>% 
  unnest(results) %>% 
  select(strap_number, term, estimate, r_squared) %>% 
  unnest(r_squared) %>% 
  select(strap_number, term, estimate, r.squared)
```

### Calculating  log(β̂ 0∗β1)
```{r}
log_beta = 
  bootstrap_results %>% 
  select(strap_number:estimate) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  janitor::clean_names() %>% 
  mutate(
    log_beta = log(intercept*tmin)
  ) %>% 
  select(strap_number, log_beta)
final_result = left_join(bootstrap_results, log_beta, by = "strap_number")
```

### Plotting the distribution of estimates
```{r}
## Plot of r-squared 
final_result %>%
  filter(term == "tmin") %>% 
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
    title = "Distribution of r squared"
  )
## Plot of log(β^0∗β^1)
final_result %>% 
  filter(term == "tmin") %>% 
  ggplot(aes(x = log_beta)) +
  geom_density() +
  labs(
    title = "Distribution of log(β^0∗β^1)"
  )
```

By looking at the plots, we can see that the distribution of r-squared appears normally distributed and slightly left skewed. The distribution of log(β^0∗β^1) is centered around 2 and also looks normally distributed with a slight left skew. 

### Identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r2 and log(β̂ 0∗β̂ 1)
```{r}
final_result %>%
  group_by(term) %>%
  filter(term == "tmin") %>%
   summarize(
    rsq_lower = quantile(r.squared, 0.025),
    rsq_upper = quantile(r.squared, 0.975),
    beta_lower = quantile(log_beta, 0.025),
    beta_upper = quantile(log_beta, 0.975)
  ) %>% 
  knitr::kable()
```

The 95% CI for r-squared is 0.8943 to 0.9272.
The 95% CI for log(β^0∗β^1) is 1.9647 to 2.0585.

